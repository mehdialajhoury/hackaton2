{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9jgMuhK902U",
        "outputId": "1c58d55c-94e6-4763-de93-0b5f5d476a67"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers datasets sentence-transformers schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTJNMyq-9s0N",
        "outputId": "1d23c16c-b495-4857-9df7-1e15d2c6aecc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/hackaton2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialisation du Pipeline CPU-Friendly...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-11-21 00:30:23] Démarrage du batch planifié...\n",
            "Chargement du dataset IMDB...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading readme: 7.81kB [00:00, 15.9MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subsampling : Conservation de 1250 exemples sur 25000 (5%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Your max_length is set to 40, but your input_length is only 26. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n--- Item 1/5 ---\n",
            "Prompt: .... this movie basks too much in its own innocenc\n",
            "Texte généré: .... this movie basks too much in its own innocencianity and the only thing that’s really good about it is that it doesn't feel like it’s all about being a child. It’s a movie that’s just that it’s all about making you want to be a child and not just looking for a mother. But it’s the movie that’s t...\n",
            "Summary: this movie basks too much in its own innocencianity and the only thing that’s really good about it is that it doesn’t feel like it’s all about being a child . the movie is just about making you want to be a . child and not just looking for a mother .\n",
            "Métriques:\n",
            "  - length_ok: True\n",
            "  - repetition_ratio: 0.5384615384615384\n",
            "  - semantic_score: 0.580538272857666\n",
            "Statut: SUCCES\n",
            "\\n--- Item 2/5 ---\n",
            "Prompt: IT IS A PIECE OF CRAP! not funny at all. during th\n",
            "Texte généré: IT IS A PIECE OF CRAP! not funny at all. during thriller week in the US.\n",
            "Summary: thriller week in the us is not funny at all .\n",
            "Métriques:\n",
            "  - length_ok: False\n",
            "  - repetition_ratio: 1.0\n",
            "  - semantic_score: 0.4797925353050232\n",
            "Statut: REJET_QUALITE (Score: 0.48)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n--- Item 3/5 ---\n",
            "Prompt: This is a thriller with a good concept, good actin\n",
            "Texte généré: This is a thriller with a good concept, good actin' a bad action, bad action. If you're a bad guy, you can be a bad guy, and, indeed, if you're a bad guy, you can be a bad guy, and, indeed, if you're a bad guy, you can be a bad guy, and, indeed, if you're a bad guy, you can be a bad guy, and, indeed...\n",
            "Summary: this thriller is a thriller with a good concept, good actin' a bad action, bad action . if you're a . bad guy, you can be an . good guy, and, indeed, if . you've got a problem, you have to get a job done . it's a great thriller with good concept .\n",
            "Métriques:\n",
            "  - length_ok: True\n",
            "  - repetition_ratio: 0.24390243902439024\n",
            "  - semantic_score: 0.7888089418411255\n",
            "Statut: REJET_QUALITE (Score: 0.79)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n--- Item 4/5 ---\n",
            "Prompt: I read the negative comments before viewing this f\n",
            "Texte généré: I read the negative comments before viewing this fiddle\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The only thing I ever like about the subject is the way it is written, and I think it's a really good thing to read it.\n",
            "It is, I think, the most interesting thing about the Fiddle is that it is actually a puzzle puzzle, and it has to be s...\n",
            "Summary: the fiddle is actually a puzzle puzzle, and it has to be solved . it's a lot of puzzle design and puzzle design that goes through the puzzle .\n",
            "Métriques:\n",
            "  - length_ok: True\n",
            "  - repetition_ratio: 0.5955056179775281\n",
            "  - semantic_score: 0.08857010304927826\n",
            "Statut: REJET_QUALITE (Score: 0.09)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n--- Item 5/5 ---\n",
            "Prompt: When we were in junior high school, some of us boy\n",
            "Texte généré: When we were in junior high school, some of us boycotted the gym for the school game and he gave us a message.\n",
            "\n",
            "\n",
            "“It was all about getting to work, and being the best,” he said. “I had to put a smile on my face and ask that you take pride in yourself.”\n",
            "A couple of months after the gym was shut down,...\n",
            "Summary: when we were in junior high school, some of us boycotted the gym for the school game . a couple of months after the gym was shut down, the school was back in the gym .\n",
            "Métriques:\n",
            "  - length_ok: True\n",
            "  - repetition_ratio: 0.6744186046511628\n",
            "  - semantic_score: 0.4413517713546753\n",
            "Statut: SUCCES\n",
            "\\nBatch terminé. 2 contenus valides générés.\n",
            "En attente du prochain job (Ctrl+C pour quitter)...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 184\u001b[39m\n\u001b[32m    181\u001b[39m         time.sleep(\u001b[32m1\u001b[39m)\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[43mstart_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 181\u001b[39m, in \u001b[36mstart_scheduler\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    180\u001b[39m     schedule.run_pending()\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     time.sleep(\u001b[32m1\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "import schedule\n",
        "import random\n",
        "from datetime import datetime\n",
        "from transformers import pipeline, set_seed\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from datasets import load_dataset\n",
        "\n",
        "class LowResourcePipeline:\n",
        "    def __init__(self):\n",
        "        print(\"Initialisation du Pipeline CPU-Friendly...\")\n",
        "\n",
        "        # 1. Génération (DistilGPT2 : ~300MB, très rapide)\n",
        "        self.generator = pipeline(\n",
        "            'text-generation',\n",
        "            model='distilgpt2',\n",
        "            device=-1 # Force CPU\n",
        "        )\n",
        "\n",
        "        # 2. Résumé (T5-Small : ~240MB, excellent ratio perf/taille)\n",
        "        self.summarizer = pipeline(\n",
        "            'summarization',\n",
        "            model='t5-small',\n",
        "            device=-1\n",
        "        )\n",
        "\n",
        "        # 3. Quality Control (MiniLM : le standard pour la similarité rapide)\n",
        "        self.qc_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
        "\n",
        "        # 4. Filtre Éthique (Liste de mots - O(1) complexity)\n",
        "        self.banned_words = [\"hate\", \"violence\", \"illegal\", \"badword\"]\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Charge et subsample 5% du dataset IMDB\"\"\"\n",
        "        print(\"Chargement du dataset IMDB...\")\n",
        "\n",
        "        # Chargement en mode streaming pour économiser la RAM si besoin,\n",
        "        # mais ici on charge tout pour le subsampling précis\n",
        "        dataset = load_dataset(\"imdb\", split=\"train\")\n",
        "\n",
        "        # Subsampling à 5% (Question 4)\n",
        "        total_rows = len(dataset)\n",
        "        subset_size = int(total_rows * 0.05)\n",
        "\n",
        "        print(f\"Subsampling : Conservation de {subset_size} exemples sur {total_rows} (5%)\")\n",
        "\n",
        "        # On utilise le temps actuel comme graine pour que le mélange soit différent à chaque fois\n",
        "        return dataset.shuffle(seed=int(time.time())).select(range(subset_size))\n",
        "\n",
        "    def ethical_filter(self, text):\n",
        "        \"\"\"Question 10: Filtre éthique basique\"\"\"\n",
        "        text_lower = text.lower()\n",
        "        for word in self.banned_words:\n",
        "            if word in text_lower:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def evaluate_quality(self, prompt, generated_text, summary):\n",
        "        \"\"\"Question 6 & 9: Évaluation et QC\"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        # A. Vérification de longueur\n",
        "        metrics['length_ok'] = len(generated_text.split()) > 20\n",
        "\n",
        "        # B. Vérification de répétition (ratio unique n-grams)\n",
        "        words = generated_text.split()\n",
        "        unique_words = set(words)\n",
        "        metrics['repetition_ratio'] = len(unique_words) / len(words) if words else 0\n",
        "\n",
        "        # C. Similarité Sémantique (Prompt vs Résumé)\n",
        "        # Si le résumé du texte généré est proche du prompt, le modèle n'a pas dévié.\n",
        "        embeddings = self.qc_model.encode([prompt, summary], convert_to_tensor=True)\n",
        "        similarity = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
        "        metrics['semantic_score'] = similarity\n",
        "\n",
        "        # Décision globale\n",
        "        is_valid = (\n",
        "            metrics['length_ok'] and\n",
        "            metrics['repetition_ratio'] > 0.5 and\n",
        "            metrics['semantic_score'] > 0.3\n",
        "        )\n",
        "\n",
        "        return is_valid, metrics\n",
        "\n",
        "    def process_item(self, prompt_text):\n",
        "        \"\"\"Exécute le pipeline pour un item\"\"\"\n",
        "        # 1. Génération\n",
        "        # On limite max_new_tokens pour la vitesse\n",
        "        gen_output = self.generator(\n",
        "            prompt_text,\n",
        "            max_new_tokens=100,\n",
        "            pad_token_id=50256,\n",
        "            truncation=True\n",
        "        )\n",
        "        # .strip() retire les espaces et sauts de ligne parasites au début/fin\n",
        "        generated_text = gen_output[0]['generated_text'].strip()\n",
        "\n",
        "        # 2. Filtre Éthique\n",
        "        if not self.ethical_filter(generated_text):\n",
        "            return None, \"REJET_ETHIQUE\"\n",
        "\n",
        "        # 3. Résumé\n",
        "        # T5 a besoin d'un prefixe parfois, mais t5-small gère bien le brut\n",
        "        sum_output = self.summarizer(\n",
        "            generated_text,\n",
        "            max_length=40,\n",
        "            min_length=5,\n",
        "            do_sample=False\n",
        "        )\n",
        "        summary_text = sum_output[0]['summary_text'].strip()\n",
        "\n",
        "        # 4. Évaluation\n",
        "        is_valid, metrics = self.evaluate_quality(prompt_text, generated_text, summary_text)\n",
        "\n",
        "        # On prépare les résultats\n",
        "        result_data = {\n",
        "            \"prompt\": prompt_text,\n",
        "            \"generated\": generated_text,\n",
        "            \"summary\": summary_text,\n",
        "            \"metrics\": metrics\n",
        "        }\n",
        "\n",
        "        if not is_valid:\n",
        "            # On retourne les données pour affichage des métriques, même en cas de rejet\n",
        "            return result_data, f\"REJET_QUALITE (Score: {metrics['semantic_score']:.2f})\"\n",
        "\n",
        "        return result_data, \"SUCCES\"\n",
        "\n",
        "    def run_batch(self):\n",
        "        \"\"\"Job principal exécuté par l'automate\"\"\"\n",
        "        print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Démarrage du batch planifié...\")\n",
        "        data = self.load_data()\n",
        "\n",
        "        # Sélection de 5 indices aléatoires distincts pour garantir la variété\n",
        "        indices = random.sample(range(len(data)), 5)\n",
        "        sample_batch = data.select(indices)\n",
        "        total_items = len(sample_batch)\n",
        "\n",
        "        results = []\n",
        "        for i, item in enumerate(sample_batch):\n",
        "            # On utilise les 50 premiers caractères comme prompt\n",
        "            prompt = item['text'][:50]\n",
        "            result, status = self.process_item(prompt)\n",
        "\n",
        "            print(f\"\\\\n--- Item {i+1}/{total_items} ---\")\n",
        "\n",
        "            if result:\n",
        "                print(f\"Prompt: {result['prompt']}\")\n",
        "\n",
        "                # Truncate affichage texte généré (300 chars max pour l'affichage)\n",
        "                gen_text = result['generated']\n",
        "                if len(gen_text) > 300:\n",
        "                    gen_text = gen_text[:300] + \"...\"\n",
        "                print(f\"Texte généré: {gen_text}\")\n",
        "\n",
        "                print(f\"Summary: {result['summary']}\")\n",
        "                print(f\"Métriques:\")\n",
        "                for key, val in result['metrics'].items():\n",
        "                    print(f\"  - {key}: {val}\")\n",
        "\n",
        "                if \"SUCCES\" in status:\n",
        "                    results.append(result)\n",
        "\n",
        "            print(f\"Statut: {status}\")\n",
        "\n",
        "        print(f\"\\\\nBatch terminé. {len(results)} contenus valides générés.\")\n",
        "\n",
        "# --- AUTOMATION (Question 8) ---\n",
        "def start_scheduler():\n",
        "    pipeline_instance = LowResourcePipeline()\n",
        "\n",
        "    # Exécuter immédiatement une fois pour tester\n",
        "    pipeline_instance.run_batch()\n",
        "\n",
        "    # Planifier toutes les heures\n",
        "    schedule.every(1).minutes.do(pipeline_instance.run_batch)\n",
        "\n",
        "    print(\"En attente du prochain job (Ctrl+C pour quitter)...\")\n",
        "    while True:\n",
        "        schedule.run_pending()\n",
        "        time.sleep(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_scheduler()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hackaton2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
